{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Processes: Seasons and Temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probabilities and Entropies 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a bi-variate process. The four seasons $\\mathbb{S} = \\{ \\text{spring}, \\text{summer}, \\text{autumn}, \\text{winter} \\}$ occur with the same probability $\\Pr \\{ S_{\\mu} = 0.25 \\}$. From our experience, the seasons are correlated to average temperatures. For simplicity, we only have three temperatures $\\mathbb{T} = \\{ -10°C, +10°C, +20°C \\}$. The reslationship between seasons and temperatures can be described by the conditional probabilities given in the table below.\n",
    "\n",
    "\n",
    "| ${\\Pr\\{ {\\cal T} \\mid {\\cal S} \\}}$ | spring | summer | autumn | winter |\n",
    "| ----- | ----- | ----- | ----- | ----- | \n",
    "| -10°C |  0.1  |  0.0  |  0.1  |  0.8  |\n",
    "| +10°C |  0.6  |  0.1  |  0.6  |  0.2  |\n",
    "| +20°C |  0.3  |  0.9  |  0.3  |   0   | \n",
    "\n",
    "Given a particular season, the three temperatures occur with different probabilities, they are not equally likely. \n",
    "\n",
    "The conditional entropy $H({\\cal T} \\mid {\\cal S})$ is defined by\n",
    "\n",
    "\\begin{align*}\n",
    "    H({\\cal T} \\mid {\\cal S})\n",
    "    &= \\mathrm{E} \\big\\{ \\log_2 \\Pr \\{ {\\cal T}=t \\mid {\\cal S}=s \\} \\big\\} \\\\\n",
    "    &= - \\sum_{s \\in \\mathbb{S}} \\sum_{t \\in \\mathbb{T}} \\Pr \\{ {\\cal T}=t, {\\cal S}=s \\} \\cdot \\log_2 \\Pr \\{ {\\cal T}=t \\mid {\\cal S}=s \\} \\\\\n",
    "&= - \\sum_{s \\in \\mathbb{S}} \\Pr\\{ {\\cal S} = s \\} \\cdot \\sum_{t \\in \\mathbb{T}} \\Pr \\{ {\\cal T}=t \\mid {\\cal S}=s \\} \\cdot \\log_2 \\Pr \\{ {\\cal T}=t \\mid {\\cal S}=s \\} \\\\\n",
    "&= - \\frac{1}{4} \\cdot  \\sum_{s \\in \\mathbb{S}} \\sum_{t \\in \\mathbb{T}} \\Pr \\{ {\\cal T}=t \\mid {\\cal S}=s \\} \\cdot \\log_2 \\Pr \\{ {\\cal T}=t \\mid {\\cal S}=s \\}\n",
    "\\end{align*}\n",
    "\n",
    "For $H({\\cal T} \\mid {\\cal J})=0$, the temperature is completely determined by the season and there is no uncertainty about the temperature if the season is known. For $H({\\cal T} \\mid {\\cal S})=H({\\cal T})$ , the uncertainty is maximized because knowing the season does not reduce the uncertainty about the temperature. In this case, seasons and temperatures will be statistically independent of each other.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conditional entropy amounts to H(T | S) = 0.945462 bit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# definition of temperature probabilities conditioned on seasons\n",
    "Pr_T_S = np.array([[0.1, 0, 0.1, 0.8],[0.6, 0.1, 0.6, 0.2],[0.3, 0.9, 0.3, 0.0]])\n",
    "\n",
    "# Computation of conditional entropy H(T | S)  (zeros are not considered due to 0 * log(0) = 0)\n",
    "H_T_S = 0.25 * Pr_T_S * np.log2(Pr_T_S, out=np.zeros_like(Pr_T_S), where=(Pr_T_S!=0))\n",
    "H_T_S = - np.sum(H_T_S)\n",
    "\n",
    "print(\"The conditional entropy amounts to H(T | S) = %g bit.\\n\" % (H_T_S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Probabilities and Joint Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint probabilities can be computed by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Pr\\{ {\\cal T}=t, {\\cal S}=s\\} = \\Pr\\{ {\\cal T}=t \\mid {\\cal S}=s\\} \\cdot \\Pr\\{ {\\cal S}=s\\}\n",
    "\\end{equation}\n",
    "\n",
    "because the season probabilitiers are known. The corresponding joint entropy becomes\n",
    "\n",
    "\\begin{align*}\n",
    "    H({\\cal T}, {\\cal S}) \n",
    "    &= \\mathrm{E} \\big\\{ \\log_2 \\Pr\\{ {\\cal T}=t, {\\cal S}=s\\} \\big\\} \\\\\n",
    "    &= - \\sum_{s \\in \\mathbb{S}} \\sum_{t \\in \\mathbb{T}} \\Pr \\{ {\\cal T}=t, {\\cal S}=s \\} \\cdot \\log_2 \\Pr \\{ {\\cal T}=t, {\\cal S}=s \\} .\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joint probabilities amount to \n",
      " [[0.025 0.    0.025 0.2  ]\n",
      " [0.15  0.025 0.15  0.05 ]\n",
      " [0.075 0.225 0.075 0.   ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computation of joint probabilities\n",
    "Pr_TS = Pr_T_S * 0.25\n",
    "\n",
    "print(\"The joint probabilities amount to \\n\", Pr_TS, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joint entropy becomes 2.94546 bit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computation of joint entropy (zeros are skipped due to 0 * log(0) = 0)\n",
    "H_TS = Pr_TS * np.log2(Pr_TS, out=np.zeros_like(Pr_TS), where=(Pr_TS!=0))\n",
    "H_TS = - np.sum(H_TS)\n",
    "\n",
    "print(\"The joint entropy becomes %g bit.\\n\" % (H_TS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Probabilities and Entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal probabilities can be computed using the relationships\n",
    "\n",
    "\\begin{align}\n",
    "    \\Pr\\{ {\\cal T}=t \\} &= \\sum_{ s \\in \\mathbb{S}} \\Pr\\{ {\\cal T}=t, {\\cal S}=s\\} \\\\\n",
    "    \\Pr\\{ {\\cal S}=s \\} &= \\sum_{ t \\in \\mathbb{T}} \\Pr\\{ {\\cal T}=t, {\\cal S}=s\\} . \n",
    "\\end{align}\n",
    "\n",
    "Therefore, the temperature probabilities are obtained by summing over the columns of the above table. Consequently, the season probabilities are determined by summing the rows of that table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temperature probabilities amount to \n",
      " [0.25  0.375 0.375]\n",
      "The temperature entropie amounts to H(T)=1.56128 bit. \n",
      "\n",
      "for validation: The season probabilities mount to \n",
      " [0.25 0.25 0.25 0.25]\n",
      "The season entropy amounts to H(S)=2 bit. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# marginal temperature probabilities \n",
    "Pr_T = np.sum(Pr_TS,axis=1)\n",
    "Pr_S = np.sum(Pr_TS,axis=0)\n",
    "\n",
    "# Entropies for temperatures and seasons\n",
    "H_T = - Pr_T @ np.transpose(np.log2(Pr_T))\n",
    "H_S = - Pr_S @ np.transpose(np.log2(Pr_S))\n",
    "\n",
    "print(\"The temperature probabilities amount to \\n\", Pr_T)\n",
    "print(\"The temperature entropie amounts to H(T)=%g bit. \\n\" %(H_T))\n",
    "\n",
    "print(\"for validation: The season probabilities mount to \\n\", Pr_S)\n",
    "print(\"The season entropy amounts to H(S)=%g bit. \\n\" %(H_S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probabilities and Entropies 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on marginal and joint probabilities, we can determine the conditional probabilities\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Pr\\{ {\\cal S} \\mid {\\cal T} \\} = \\frac{\\Pr\\{ {\\cal T},  {\\cal S} \\}} {\\Pr\\{ {\\cal T} \\}} .\n",
    "\\end{equation}\n",
    "\n",
    "They describe with which probabilitiy a season occurs for a given temperature.\n",
    "\n",
    "The conditional entropy $H({\\cal S} \\mid {\\cal T})$ becomes\n",
    "\n",
    "\\begin{align*}\n",
    "    H({\\cal S} \\mid {\\cal T}) \n",
    "    &= \\mathrm{E} \\big\\{ \\log_2 \\Pr\\{ {\\cal S} \\mid {\\cal T} \\} \\big\\} \\\\\n",
    "    &= - \\sum_{s \\in \\mathbb{S}} \\sum_{t \\in \\mathbb{T}} \\Pr \\{ {\\cal T}=t, {\\cal S}=s \\} \\cdot \\log_2 \\Pr \\{ {\\cal S}=s \\mid {\\cal T}=t \\} \\\\\n",
    "    &= - \\sum_{t \\in \\mathbb{T}} \\Pr\\{ {\\cal T} = t \\} \\cdot \\sum_{s \\in \\mathbb{S}} \\Pr \\{ {\\cal S}=s \\mid {\\cal T}=t \\} \\cdot \\log_2 \\Pr \\{ {\\cal S}=s \\mid {\\cal T}=t \\} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conditional probabilities Pr{S | T} amount to \n",
      " [[0.1        0.         0.1        0.8       ]\n",
      " [0.4        0.06666667 0.4        0.13333333]\n",
      " [0.2        0.6        0.2        0.        ]] \n",
      "\n",
      "The conditional entropy H(S | T) amounts to 1.38418 bit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute conditional probabilities\n",
    "Pr_S_T = Pr_TS / np.tile(np.reshape(Pr_T,(3, 1)),(1,4))\n",
    "\n",
    "print(\"The conditional probabilities Pr{S | T} amount to \\n\", Pr_S_T, \"\\n\")\n",
    "\n",
    "# Computation of conditional entropy H( S | T)  (zeros are not considered due to 0 * log(0) = 0)\n",
    "H_S_T = Pr_TS * np.log2(Pr_S_T, out=np.zeros_like(Pr_S_T), where=(Pr_S_T!=0))\n",
    "H_S_T = - np.sum(H_S_T)\n",
    "\n",
    "print(\"The conditional entropy H(S | T) amounts to %g bit.\\n\" %(H_S_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison of the conditional entropies $H({\\cal T} | {\\cal S})$ and $H({\\cal S} | {\\cal T})$ with the uncoditional entropies $H({\\cal T})$ and $H({\\cal T})$ discloses that side information (conditioning) can reduce uncertainty. For instance, the temperatures entropy reduces from 1.56 bit to 0.945 bit whenn the season is known. For the seasons, the entropy reduces from 2 bit down to 1.38 bit for given temperature. From this artificial example, we might conclute that the temperature provides less information about the season as vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutual information represents the common information of two processes, in our example seasons and temperatures. It is defined as\n",
    "\n",
    "\\begin{align}\n",
    "    I({\\cal T};{\\cal S})\n",
    "    &= H({\\cal T}) + H({\\cal S}) - H({\\cal T},{\\cal S})\n",
    "     = H({\\cal T}) - H({\\cal T} \\mid {\\cal S})\n",
    "     = H({\\cal S}) - H({\\cal S} \\mid {\\cal T}) .\n",
    "\\end{align}\n",
    "\n",
    "The larger the mutual information the more similar are the considered processes. Thereby, the mutual information is bounded to above by the minimum of the entropies $H({\\cal S})$ and $H({\\cal T})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mutual information amounts to I(S;T)=0.615816 bit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "I_ST = 2 - H_S_T\n",
    "\n",
    "print(\"The mutual information amounts to I(S;T)=%g bit.\\n\" %(I_ST))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
